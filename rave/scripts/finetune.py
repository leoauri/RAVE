import hashlib
import pdb
import multiprocessing
import os
import sys
from pathlib import Path
from typing import Any, Dict

import gin
import pytorch_lightning as pl
import torch
from absl import flags, app, logging
from torch.utils.data import DataLoader

try:
    import rave
except:
    import sys, os 
    sys.path.append(os.path.abspath('.'))
    import rave

import acids_dataset as ad
ad.fragments.base.FORCE_ARRAY_RESHAPE = False

import rave
import rave.core
import rave.dataset
from rave.dataset import get_augmentations, parse_transform


FLAGS = flags.FLAGS

flags.DEFINE_string('name', None, help='Name of the run', required=True)
flags.DEFINE_string('run', None, help='Path of model to fine-tune', required=True)
flags.DEFINE_string('db_path',
                    None,
                    help='Preprocessed dataset path',
                    required=True)
flags.DEFINE_string('out_path',
                    default="runs/",
                    help='Output folder')
flags.DEFINE_multi_string('augment',
                           default = [],
                            help = 'augmentation configurations to use')
flags.DEFINE_multi_string('train',
                           default = ["all"],
                           help = 'specify weights to train')
flags.DEFINE_multi_string('freeze',
                           default = [],
                           help = 'specify weights to freeze')                           
flags.DEFINE_integer('max_steps',
                     6000000,
                     help='Maximum number of training steps')
flags.DEFINE_integer('val_every', 10000, help='Checkpoint model every n steps')
flags.DEFINE_integer('save_every',
                     500000,
                     help='save every n steps (default: just last)')
flags.DEFINE_integer('seed',
                      default = 0,
                      help = 'augmentation configurations to use')                    
flags.DEFINE_integer('n_signal',
                     131072,
                     help='Number of audio samples to use during training')
flags.DEFINE_integer('batch', 8, help='Batch size')
flags.DEFINE_string('ckpt',
                    None,
                    help='Path to previous checkpoint of the run')
flags.DEFINE_multi_string('override', default=[], help='Override gin binding')
flags.DEFINE_integer('workers',
                     default=0,
                     help='Number of workers to spawn for dataset loading')
flags.DEFINE_multi_string('device', default="auto", help="training device (default: auto. Can be cuda, cuda:0, ..., mps, etc.)")
flags.DEFINE_bool('derivative',
                  default=False,
                  help='Train RAVE on the derivative of the signal')
flags.DEFINE_bool('normalize',
                  default=False,
                  help='Train RAVE on normalized signals')
flags.DEFINE_list('rand_pitch',
                  default=None,
                  help='activates random pitch')
flags.DEFINE_bool('ema_weights',
                  default=False,
                  help='Use ema weights if avaiable')
flags.DEFINE_bool('progress',
                  default=True,
                  help='Display training progress bar')
flags.DEFINE_bool('reset_discriminator', default=False, help="resets discriminator")
flags.DEFINE_bool('smoke_test', 
                  default=False,
                  help="Run training with n_batches=1 to test the model")
flags.DEFINE_bool('allow_partial_resume', default=False, help="allow partial resuming of a checkpoint")


def add_gin_extension(config_name: str) -> str:
    if config_name[-4:] != '.gin':
        config_name += '.gin'
    return config_name

def parse_augmentations(augmentations, sr):
    for a in augmentations:
        with rave.core.GinEnv(paths=[Path(rave.__file__).parent / "configs" / "augmentations"]):
            gin.parse_config_file(a)
            parse_transform()
            gin.clear_config()
    return get_augmentations()

def main(argv):
    torch.set_float32_matmul_precision('high')
    torch.manual_seed(FLAGS.seed)
    torch.backends.cudnn.benchmark = True

    # parse configuration
    if FLAGS.reset_discriminator: 
        remove_keys = ["discriminator.*"]
    else:
        remove_keys = None
    try:
        model, run_path = rave.load_rave_checkpoint(FLAGS.run, ema=FLAGS.ema_weights, remove_keys=remove_keys)
    except FileNotFoundError:
        model, run_path = rave.load_rave_checkpoint(FLAGS.run, ema=FLAGS.ema_weights, name=None, remove_keys=remove_keys)
    RUN_NAME = rave.get_run_name(run_path)

    if FLAGS.derivative:
        #TODO replace with transform
        model.integrator = rave.dataset.get_derivator_integrator(model.sr)[1]
    gin.constant('SAMPLE_RATE', model.sr)
    
    # parse augmentations
    with gin.unlock_config():
        augmentations = parse_augmentations(map(add_gin_extension, FLAGS.augment), sr=model.sr)
        gin.bind_parameter('dataset.get_dataset.augmentations', augmentations)

    # parse keys to train / freeze 
    training_keys = rave.core.get_finetune_keys(model, FLAGS.train, FLAGS.freeze)
    if len(training_keys) == 0:
        logging.error('No training keys found with train=%s, freeze=%s'%(FLAGS.train, FLAGS.freeze))
    logging.info("Number of parameters fine-tuned : %d"%len(training_keys))
    gin.bind_parameter("rave.configure_optimizers.weight_list", training_keys)

    # parse datasset
    dataset = rave.dataset.get_dataset(FLAGS.db_path,
                                       model.sr,
                                       FLAGS.n_signal,
                                       derivative=FLAGS.derivative,
                                       normalize=FLAGS.normalize,
                                       rand_pitch=FLAGS.rand_pitch,
                                       augmentations=augmentations,
                                       n_channels=model.n_channels)

    train, val = rave.dataset.split_dataset(dataset, percent=98, training_name=RUN_NAME)

    num_workers = rave.core.get_workers(FLAGS.workers)
    train = DataLoader(train,
                       FLAGS.batch,
                       True,
                       drop_last=True,
                       num_workers=num_workers)
    val = DataLoader(val, FLAGS.batch, False, num_workers=num_workers)


    # CHECKPOINT CALLBACKS
    validation_checkpoint = pl.callbacks.ModelCheckpoint(monitor="validation",
                                                         filename="best")
    last_filename = "last" if FLAGS.save_every is None else "epoch-{epoch:04d}"                                                        
    last_checkpoint = rave.core.ModelCheckpoint(filename=last_filename, step_period=FLAGS.save_every)

    val_check = {}
    if len(train) >= FLAGS.val_every:
        val_check["val_check_interval"] = 1 if FLAGS.smoke_test else FLAGS.val_every
    else:
        nepoch = FLAGS.val_every // len(train)
        val_check["check_val_every_n_epoch"] = nepoch

    if FLAGS.smoke_test:
        val_check['limit_train_batches'] = 1
        val_check['limit_val_batches'] = 1

    os.makedirs(os.path.join(FLAGS.out_path, RUN_NAME), exist_ok=True)

    accelerator, devices = rave.core.get_training_device(FLAGS.device, allow_multi=True)
    
    callbacks = [
        validation_checkpoint,
        last_checkpoint,
        rave.model.WarmupCallback(),
        rave.model.QuantizeCallback(),
        rave.model.BetaWarmupCallback(),
    ]

    run = None
    if FLAGS.ckpt:
        run = rave.core.search_for_run(FLAGS.ckpt)
        if run is None: 
            run = rave.core.search_for_run(FLAGS.ckpt, None)
            if run is None:
                logging.error('could not find model with ckpt=%s. Maybe provide a more detailed path: %s'%FLAGS.ckpt)
                exit()

    trainer = pl.Trainer(
        logger=pl.loggers.TensorBoardLogger(
            FLAGS.out_path,
            name=RUN_NAME,
        ),
        accelerator=accelerator,
        devices=devices,
        callbacks=callbacks,
        max_epochs=300000,
        max_steps=FLAGS.max_steps,
        profiler="simple",
        enable_progress_bar=FLAGS.progress,
        log_every_n_steps=min(30, len(dataset)),
        **val_check,
    )


    with open(os.path.join(FLAGS.out_path, RUN_NAME, "config.gin"), "w") as config_out:
        config_out.write(gin.operative_config_str())

    trainer.fit(model, train, val, ckpt_path=run)


if __name__ == "__main__": 
    app.run(main)
